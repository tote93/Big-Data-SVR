{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SVR_Implementation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5-final"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vrkjaul-wk2l",
        "colab": {}
      },
      "source": [
        "# Install svn package to download folder\n",
        "package = ! dpkg -s subversion | grep Status\n",
        "if \"Status: install ok installed\" not in package:\n",
        "  !sudo apt-get install subversion\n",
        "# Download dataset folders from github\n",
        "# Install svn package to download folder\n",
        "package = ! dpkg -s subversion | grep Status\n",
        "if \"Status: install ok installed\" not in package:\n",
        "  !sudo apt-get install subversion\n",
        "# Download dataset folders from github\n",
        "! svn checkout https://github.com/tote93/Big-Data-SVR/trunk/dataset/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Password:\n   C dataset/original\n   A dataset/original/CO2 march.xls\n   A dataset/original/co2 april.xls\n   A dataset/original/humidity april.xls\n   A dataset/original/humidity march.xls\n   A dataset/original/temperature1 april.xls\n   A dataset/original/temperature1 march.xls\n   A dataset/original/temperature2 april.xls\n   A dataset/original/temperature2 march.xls\nRevisi√≥n obtenida: 3\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "skgEnUU84vpL"
      },
      "source": [
        "# 1. Import the Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "colab_type": "code",
        "id": "rmH0Jp3s4y1I",
        "outputId": "34c1dfff-84e0-480a-a7c2-640a0de4817f"
      },
      "outputs": [],
      "source": [
        "#To handle data in the form of rows and columns\n",
        "import pandas as pd\n",
        "#For numerical libraries\n",
        "import numpy as np\n",
        "# Import Path for move files\n",
        "from pathlib import Path\n",
        "# Avoid show warning msg\n",
        "import warnings\n",
        "# Measure time and generate files libraries\n",
        "import os, time\n",
        "# Check if file has a word in filename\n",
        "import fnmatch \n",
        "# Sklearn libraries for ml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "# Sklearn libraries for testint models\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn import linear_model\n",
        "# Sklearn standarScale libraries\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.metrics import r2_score, mean_squared_error, make_scorer\n",
        "# Support Vector Regressor library\n",
        "from sklearn.svm import SVR\n",
        "# Graphic and plot libraries\n",
        "from matplotlib import pyplot\n",
        "#importing ploting libraries\n",
        "import matplotlib.pyplot as plt\n",
        "#To enable plotting graphs in Jupyter notebook\n",
        "%matplotlib inline\n",
        "#importing seaborn for statistical plots\n",
        "import seaborn as sns\n",
        "# Plot and seaborn configuration\n",
        "plt.rc('font',size=14)\n",
        "sns.set(style='white')\n",
        "sns.set(style='whitegrid',color_codes=True)\n",
        "warnings.filterwarnings('ignore')\n",
        "# Add posibles missing values to search in the dataset\n",
        "missing_values = [\"n/a\", \"na\", \"--\", \"?\", \"NaN\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab_type": "text",
        "id": "_eLY5ZsXH0Wc"
      },
      "outputs": [],
      "source": [
        "# Processing all dataset files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H6-hVogfH-Dn",
        "colab": {}
      },
      "source": [
        "def datetimeClean(data):\n",
        "    dti = pd.to_datetime(data['Date'])\n",
        "    data['Date'] = dti.dt.round(\"T\")\n",
        "    # Replace all seconds to 0, to start all in the same time\n",
        "    data['Date'] = data['Date'].apply(lambda t: t.replace(second=0))\n",
        "    data.drop_duplicates(subset =\"Date\", keep = \"first\", inplace = True)\n",
        "    return data"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xjs8yrnAIGW8",
        "colab": {}
      },
      "source": [
        "# Read data from file\n",
        "for file in os.listdir(\"./dataset/original\"):\n",
        "    if fnmatch.fnmatch(file, '*'):\n",
        "        # create  dataframe from excel\n",
        "        data = pd.read_excel(\"./dataset/original/\"+file)\n",
        "        # Clean datatime, round to next minute\n",
        "        data = datetimeClean(data)\n",
        "        # Create the final dataframe with the original columns\n",
        "        df_final = pd.DataFrame(columns = data.columns)\n",
        "        # Reformating Datetime \n",
        "        end_month = pd.to_datetime(data['Date'][len(data)-1]).month\n",
        "        df = data\n",
        "        # Reformate datetime value\n",
        "        df['Date'] = df['Date'].dt.strftime('%d-%m-%Y %H:%M:%S')\n",
        "        # Transform date from string to datetime type\n",
        "        df['Date'] = pd.to_datetime(df['Date'])\n",
        "        # set the time column as index\n",
        "        df = df.set_index(['Date'])\n",
        "        # target: date_range 1 min steps\n",
        "        tgt = pd.date_range(df.index[0], df.index[-1], freq='T')\n",
        "        # now use .isin() and negate to get the timestamps that are missing in df\n",
        "        missing_data = tgt[~tgt.isin(df.index)]\n",
        "        # Append all missing data\n",
        "        for date in missing_data:\n",
        "            data = data.append(pd.Series([date, np.nan], index=data.columns), ignore_index=True)\n",
        "\n",
        "        # Parse data to datetime, could be more efficient using a dictionary, loop O(N)\n",
        "        data['Date'] = pd.to_datetime(data['Date'])\n",
        "        for i, _date in enumerate(data['Date']):      \n",
        "            # Ignore dates out the initial range\n",
        "            if pd.to_datetime(_date).month == end_month:                  \n",
        "                df_final = df_final.append(pd.Series([_date, round(data['Y_Value'][i],2)], index=data.columns), ignore_index=True)\n",
        "        # Sort values and export\n",
        "        df_final = df_final.sort_values(by=['Date'])\n",
        "        # Export to excel file\n",
        "        df_final.to_excel(file, index=False)\n",
        "        print(\"exported:\",file, \"NumPatrones:\",len(df_final))        \n",
        "print(\"Finished exporting files\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "exported: temperature1 april.xls NumPatrones: 35418\nexported: humidity april.xls NumPatrones: 35418\nexported: CO2 march.xls NumPatrones: 38412\nexported: humidity march.xls NumPatrones: 38411\nexported: co2 april.xls NumPatrones: 35418\nexported: temperature1 march.xls NumPatrones: 38410\nexported: temperature2 march.xls NumPatrones: 38411\nexported: temperature2 april.xls NumPatrones: 35418\nFinished exporting files\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wqMJyXH1I2OS"
      },
      "source": [
        "# Compute all new files to merge into a month file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "D_dORZsuJAsd",
        "colab": {}
      },
      "source": [
        "os.makedirs('file_output')\n",
        "os.makedirs('dataset/final')\n",
        "# Open all files from march\n",
        "df1 = pd.read_excel(\"temperature1 march.xls\")\n",
        "df2 = pd.read_excel(\"temperature2 march.xls\")\n",
        "df3 = pd.read_excel(\"humidity march.xls\")\n",
        "df4 = pd.read_excel(\"CO2 march.xls\")\n",
        "# Rename column and add new colums from other datasets\n",
        "df1 = df1.rename(columns={\"Y_Value\": \"Temp1\"})\n",
        "df1['Temp2'] = df2['Y_Value']\n",
        "df1['Humi'] = df3['Y_Value']\n",
        "df1['CO2'] = df4['Y_Value']\n",
        "# Finally export to a new dataset\n",
        "df1.to_csv(\"file_output/march_output.csv\", index=False, header=True, decimal=\".\", sep=\",\", float_format='%.2f')\n",
        "\n",
        "# Open all files from april\n",
        "df1 = pd.read_excel(\"temperature1 april.xls\")\n",
        "df2 = pd.read_excel(\"temperature2 april.xls\")\n",
        "df3 = pd.read_excel(\"humidity april.xls\")\n",
        "df4 = pd.read_excel(\"co2 april.xls\")\n",
        "# Rename column and add the other attirbutes\n",
        "df1 = df1.rename(columns={\"Y_Value\": \"Temp1\"})\n",
        "df1['Temp2'] = df2['Y_Value']\n",
        "df1['Humi'] = df3['Y_Value']\n",
        "df1['CO2'] = df4['Y_Value']\n",
        "# Export to a new dataset\n",
        "df1.to_csv(\"file_output/april_output.csv\", index=False, header=True, decimal=\".\", sep=\",\", float_format='%.2f')\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xVHYYRWnJFQp",
        "colab": {}
      },
      "source": [
        "# Clean all secondary files\n",
        "dir_name = \".\"\n",
        "test = os.listdir(dir_name)\n",
        "for item in test:\n",
        "    if item.endswith(\".xls\"):\n",
        "        os.remove(os.path.join(dir_name, item))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FcRFDejAJngW"
      },
      "source": [
        "# Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EGWDNB6vMe0e",
        "colab": {}
      },
      "source": [
        "#reading the CSV file into pandas dataframe\n",
        "data = pd.read_csv(\"file_output/april_output.csv\", na_values = missing_values, sep=\",\", parse_dates=True)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MOwgs3KnSTNs",
        "colab": {}
      },
      "source": [
        "#Check top few records of the dataset\n",
        "data.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "                  Date  Temp1  Temp2   Humi     CO2\n0  2019-04-01 02:00:00  24.66   23.3  27.84  424.96\n1  2019-04-01 02:01:00  24.64   23.3  27.84  424.96\n2  2019-04-01 02:02:00  24.66   23.3  27.84  422.72\n3  2019-04-01 02:03:00  24.64   23.3  27.84  421.76\n4  2019-04-01 02:04:00  24.64   23.3  27.84  419.84",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>Temp1</th>\n      <th>Temp2</th>\n      <th>Humi</th>\n      <th>CO2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2019-04-01 02:00:00</td>\n      <td>24.66</td>\n      <td>23.3</td>\n      <td>27.84</td>\n      <td>424.96</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2019-04-01 02:01:00</td>\n      <td>24.64</td>\n      <td>23.3</td>\n      <td>27.84</td>\n      <td>424.96</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2019-04-01 02:02:00</td>\n      <td>24.66</td>\n      <td>23.3</td>\n      <td>27.84</td>\n      <td>422.72</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2019-04-01 02:03:00</td>\n      <td>24.64</td>\n      <td>23.3</td>\n      <td>27.84</td>\n      <td>421.76</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2019-04-01 02:04:00</td>\n      <td>24.64</td>\n      <td>23.3</td>\n      <td>27.84</td>\n      <td>419.84</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "j1HOSIxPSZdB"
      },
      "source": [
        "*   It shows that there are four independent variables ( Date, Temp1, Temp2, Humi) and one dependent variable (CO2).\n",
        "*   All the records are numeric but Date attribute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WKehe83eSxTv",
        "colab": {}
      },
      "source": [
        "#Check the last few records of the dataset\n",
        "data.tail()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "                      Date  Temp1  Temp2  Humi     CO2\n35413  2019-04-25 16:13:00  27.64   26.9  34.9  406.72\n35414  2019-04-25 16:14:00  27.62   26.9  34.9  405.76\n35415  2019-04-25 16:15:00  27.62   26.9  34.9  406.72\n35416  2019-04-25 16:16:00  27.62   26.9  34.9  408.00\n35417  2019-04-25 16:17:00  27.62   26.9  34.9  404.80",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>Temp1</th>\n      <th>Temp2</th>\n      <th>Humi</th>\n      <th>CO2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>35413</th>\n      <td>2019-04-25 16:13:00</td>\n      <td>27.64</td>\n      <td>26.9</td>\n      <td>34.9</td>\n      <td>406.72</td>\n    </tr>\n    <tr>\n      <th>35414</th>\n      <td>2019-04-25 16:14:00</td>\n      <td>27.62</td>\n      <td>26.9</td>\n      <td>34.9</td>\n      <td>405.76</td>\n    </tr>\n    <tr>\n      <th>35415</th>\n      <td>2019-04-25 16:15:00</td>\n      <td>27.62</td>\n      <td>26.9</td>\n      <td>34.9</td>\n      <td>406.72</td>\n    </tr>\n    <tr>\n      <th>35416</th>\n      <td>2019-04-25 16:16:00</td>\n      <td>27.62</td>\n      <td>26.9</td>\n      <td>34.9</td>\n      <td>408.00</td>\n    </tr>\n    <tr>\n      <th>35417</th>\n      <td>2019-04-25 16:17:00</td>\n      <td>27.62</td>\n      <td>26.9</td>\n      <td>34.9</td>\n      <td>404.80</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "a8j9wEzbS13h"
      },
      "source": [
        "# 3. Exploratory data quality report April Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VDbketCfZ6yD"
      },
      "source": [
        ">## 3.2 Multivariate analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jQ9vSTWGgzXK"
      },
      "source": [
        "> ## 3.3. Strategies to handle different data challenges"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pYEeWqmrhcnf"
      },
      "source": [
        "#### Checking for Missing Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "R4Dis5DrnT7f"
      },
      "source": [
        "**Handling the outliers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "U1oiKxBsiB8q"
      },
      "source": [
        "#### Preprocessing datasets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kOynDf2er5ra"
      },
      "source": [
        "#### Output Dataset Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rHIx6urWsjCD"
      },
      "source": [
        "# SVR Parameters Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LlGRUrG-t_zh"
      },
      "source": [
        "## **Parameters to optimice**\n",
        "**C (Regularisation)**: C is the penalty parameter. The higher the Penalty value the model makes fewer mistakes but it is observed that after a certain Penalty value the model is over training.\n",
        "\n",
        "**Gamma**: It defines how far influences the calculation of plausible line of separation. The higher the gamma value, only near points are considered.\n",
        "A lower gamma value, far away points are also considered.\n",
        "\n",
        "**Degree**: Higher degree polynomial kernels allow a more flexible decision boundary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ihe9QctmzDjc"
      },
      "source": [
        "## Optimizing Parameters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVu-ldGntmDa",
        "colab_type": "text"
      },
      "source": [
        "### Generate Algorithms Comparison\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUBG2mt0ukYd",
        "colab_type": "text"
      },
      "source": [
        "### Testing with the full dataset preprocesed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEQWQLc4cWA7",
        "colab_type": "text"
      },
      "source": [
        "### Divide dataset in train - test files and preprocess individually\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PI17jq2ifS3d",
        "colab_type": "text"
      },
      "source": [
        "###¬†Now we Fit the model of previous preprocesed\n"
      ]
    }
  ]
}